# 聚类分析

> 聚类分析是根据在数据中发现的描述对象及其关系的信息，将数据对象分组。目的是，组内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。组内相似性越大，组间差距越大，说明聚类效果越好。

传统聚类分析方法大多是基于数据的距离或者数据的相关性，从而进行聚类的，即将距离相近的数据归为一类，或是将相关程度较高的数据归为一类；因此，根据上一章[相关（一致性）分析](xiang-guan-fen-xi.md)，能进行相关性分析的数据是都能够进行数据聚类的，主要包括顺序变量（学生的分数）、名义变量（包括分类数据）以及等距变量（学生成绩的排名）

{% hint style="info" %}
这里简单介绍一下名义变量中的分类数据，学生的选择题作答情况便可以看作是分类数据，例如，对于T1，学生A选择了A，学生B选择了B，那么仅对于T1，学生A的向量类型便是\[1,0,0,0\]，学生B的向量类型便是\[0,1,0,0\]，通过这种方式，将学生的作答情况转换为\[1,0,0,0,0,1,0,0,...\]这种向量形式，从而进行两个向量之间的距离运算，根据学生的选择题作答情况进行聚类。
{% endhint %}

常见的聚类分析算法有很多，在此仅介绍K-means聚类算法。

### K-means聚类算法

k‐means聚类是基于划分的聚类算法，计算样本点与类簇质心的距离，与类簇质心相近的样本点划分为同一类簇。k‐均值通过样本间的距离来衡量它们之间的相似度，两个样本距离越远，则相似度越低，否则相似度越高，因此，我认为K-means聚类算法所适用的数据类型是数值型或是能够进行欧式距离计算的数据，其算法流程为：

1. 选择K 个初始质心\(K需要指定），初始质心随机选择即可，**每一个质心为一个类**
2. 对剩余的每个样本点，计算它们到各个质心的欧式距离，并将其归入到相互间距离最小的质心所在的簇，计算各个新簇的质心
3. 在所有样本点都划分完毕后，根据划分情况重新计算各个簇的质心所在位置，然后迭代计算各个样本点到各簇质心的距离，对所有样本点重新进行划分
4. 重复2和3，直到质心不再发生变化时或者达到最大迭代次数时，算法终止。

#### K-means聚类算法的优点和缺点

* 算法原理简单，容易实现，且运行效率比较高（优点）
* 聚类结果容易解释，适用于高维数据的聚类（优点）
* 采用贪心策略，导致容易局部收敛（缺点）；在这里说明一下这个局部收敛，根据K-means聚类算法的流程，“当质心不再发生变化”时便认为聚类完成，由于算法采用了贪心策略，质心会在离它最近的点中进行质心的更新迭代，因此对于某些质心，它有可能会在局部完成达到最优，而对于全局来说，此刻的质心并非最优的质心；这也导致了算法另一个缺点，初始聚类中心的选取也对算法结果影响很大，不同的初始中心可能会导致不同的聚类结果。
* 算法对离群点和噪声点非常敏感，少量的离群点和噪声点可能对算法求平均值产生极大影响，从而影响聚类结果（缺点）

因此，根据上述的K-means聚类算法的缺点，再进行K-means算法进行**分数线的聚类划分**时，需要先进行异常值的检测以及缺失值的处理，详见[数据的清洗](untitled-1.md)



